{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n",
    "We're going to continue working with CAP imagery for the second half of this week. Recall the two main guiding questions for this week:\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?\n",
    "\n",
    "## Motivation\n",
    "\n",
    "We've already seen how structure from motion can begin to answer the second of those questions, at least in a coarse manner. The first question is more difficult than the second for one crucial reason: it (so far) can not be answered *solely* using image data. Recall that structure from motion was able to leverage image metadata, features and geometric constraints without having to insert any \"outside\" data. When detecting the components of an image, there is a certain amount of subjectivity that so far requires human input. For example, if you want to detect flooding in an image, there need to be some set of rules that the software can determine \"if (rules are satisfied), then (flooding = True)\".\n",
    "\n",
    "How can you start to tackle this problem? We're going to make the problem a bit simpler by turning it into a classification problem rather than a localization problem. That is, instead of finding where flooding is in an image, we'll find whether there is flooding at all. With that in mind, let's first try a naive approach and see if we can simply enumerate the rules. Let's look at some flooding images:\n",
    "\n",
    "<img src=\"notebook_images/010_1282_99809b0c-fc64-46e5-957f-7ff8e7547d8c.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/DSC_0655_0cbbe7e5-be24-4274-bf5a-664c6f2dc3e1.jpg\" width=\"500\"  />\n",
    "\n",
    "So what do we see? It looks like flooding is this murky, brown color that covers most of the image. So let's make a rule: if some percentage of the image is this brownish color (you can think of detecting this by creating some sort of index, like you did with the satellite imagery), there is flooding in the image. This might work for the two images we saw, but the real test is seeing if it works with other images. So let's look at another one:\n",
    "\n",
    "<img src=\"notebook_images/DSC_8218_7bb15c80-481b-411a-a54a-a36e9d8b6098.jpg\" width=\"500\"  />\n",
    "\n",
    "This image has a large percentage of it covered by brownish looking water. However, it's clearly not flooding, but rather just a lake. No problem, let's just make another rule: if some percentage of the image is this brownish color *and* there are also lots of buildings (let's also make an index for that), there is flooding in the image. Let's see if this works:\n",
    "\n",
    "<img src=\"notebook_images/DSC_8192_fe6812cd-c985-4e59-a62c-04a883ec42cc.jpg\" width=\"500\"  />\n",
    "\n",
    "Again, this is just some lake (the same lake as before, in fact). At this point, we might be thinking of shifting our strategy a bit. Clearly having someone sit down and enumerate all of the rules is impractical in this case\\*. There is simply too much variability in the images to come up with a set of rules that is remotely generalizable. \n",
    "\n",
    "This is one of the major motivations of machine learning. The idea is that, rather than enumerate the rules that determine flooding, we're going to just enumerate the outcomes (e.g. whether there is flooding or not) and develop algorithms for the computer to *learn* what the rules are. There is a rich literature (and a host of undergraduate and graduate courses) on how these algorithms are designed, but on these next couple of days we will instead focus on how to interpret and implement them. \n",
    "\n",
    "\\* This isn't to say that enumerating the rules isn't practical in *every* case. It really depends on the characteristics of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple case study\n",
    "\n",
    "We're going to start with a simple dataset. We will use the  Breast Cancer Wisconsin (Diagnostic) Data Set (https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)), a dataset that shows whether a tumor is benign or malign and various other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the cell below returns an error related to \"as_frame\" not working,\n",
    "# you need to update scikit-learn. To do this, uncomment the next\n",
    "# line and restart the kernel\n",
    "!pip install scikit-learn==0.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# read the dataframe\n",
    "df = load_breast_cancer(return_X_y=False, as_frame=True)[\"data\"]\n",
    "df[\"target\"] = load_breast_cancer(return_X_y=False, as_frame=True)[\"target\"] # benign is 1, malign is 0\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we're just going to focus on mean radius and mean texture.\n",
    "# we'll plot one against the other, and color code depending on survival\n",
    "df_b = df[df[\"target\"] == 1]\n",
    "df_m = df[df[\"target\"] == 0]\n",
    "\n",
    "# plotting\n",
    "plt.plot(df_b[\"mean radius\"], df_b[\"mean texture\"], \"ro\")\n",
    "plt.plot(df_m[\"mean radius\"], df_m[\"mean texture\"], \"bo\")\n",
    "plt.xlabel(\"mean radius\")\n",
    "plt.ylabel(\"mean texture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we see? Clearly there are factors beyond the ones plotted that explain whether a tumor is benign. However, it's not difficult to see a pattern here. The further up and to the right you are, the less likely you are to survive. So as a first attempt, we will have the software learn a *linear classifier*. This classifier will be a line that will classify you as benign if you are below the line, and malign otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from mlxtend.plotting import plot_decision_regions\n",
    "except:\n",
    "    !pip install mlxtend\n",
    "    from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# setting the features and the targets\n",
    "X = df[[\"mean radius\", \"mean texture\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# splitting training and testing set 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "# Fit data\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "# Printing parameters\n",
    "coef = classifier.coef_\n",
    "print(coef)\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_regions(X_train.values, y_train.values.astype(np.integer), clf=classifier, legend=2)\n",
    "plt.xlabel(\"mean radius\")\n",
    "plt.ylabel(\"mean texture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot going on here, so let's try to break it down. Recall that we are trying to determine whether a patient survives using just the (normalized) age and detected nodes. These are called the explanatory variables or *features*, and we refer to it as $X$. The variable we are trying to predict is the prediction variable or *target*, and we refer to it as $y$. \n",
    "\n",
    "The underlying notion behind machine learning is that patterns that hold in a subset of data will hold in the population as a whole. So in a sense, we care about the performance of the classifier on data that the algorithm has *not* seen by learning patterns in that data that we have seen. To this end, one of the major paradigms in machine learning is to separate our data into a *training set* and a *testing set*. The training set is a subset of the given data that we will actually provide to the algorithm to learn, while the testing set will be used solely to report the performance on data that the algorithm has not seen. In our case, we set 80% of the data as training data and 20% as testing data.\n",
    "\n",
    "There are numerous algorithms that can train on data. One of the more popular ones (especially before neural networks were introduced) are called *support vector machines* (SVM). We will not discuss how it actually does the learning, but suffice to say it takes in the training data and finds the line that minimizes the classification error. This line is called the *decision boundary* and it is defined by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "C_1(mean\\ radius) + C_2(mean\\ texture) = 0\n",
    "\\end{equation}\n",
    "\n",
    "Where $C_1$ and $C_2$ are the coefficients of the linear classifier. So really, the function of the SVM is to find these two coefficients. The algorithm then does classification as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{pred} =\n",
    "\\begin{cases}\n",
    "    1               & \\text{if } C_1(mean\\ radius) + C_2(mean\\ texture)\\geq 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Exercise\n",
    "- Look at the first 5 samples in the testing data set. Calculate the predicted target of each of the 5 samples?\n",
    "- Look at the corresponding targets. Are the predictions accurate? What proportion of the predicted values are correctly predicted?\n",
    "- Which of the two coefficients is most important? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our classifier? We can look at the performance of our classifier on the testing set to find out. Of course, an important metric to look out for is the accuracy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\#\\ (y_{true} == y_{pred})}{\\# \\ \\text{samples}}\n",
    "\\end{equation}\n",
    "\n",
    "However, for most applications it is also worth understanding what your *false positive* and *false negative* rate is. A false positive is a sample that is classified as a 1 but is in reality a 0, and a false negative is the opposite\\*. The false positive rate is defined as $\\frac{\\# \\text{False positives}}{\\# \\ \\text{False positives + True negatives}}$, while the false negative rate is defined as $\\frac{\\# \\text{False negatives}}{\\# \\ \\text{False negatives + True positives}}$. **Why would this be a metric we care about, especially in this case?**\n",
    "\n",
    "<details>\n",
    "    <summary>ANSWER</summary>\n",
    "    Imagine your tumor is predicted as malignant but is in fact benign. There is certainly now a cost you incur in further tests or surgeries, but ultimately it is a manageable cost. Now imagine if you predicted a tumor as benign even thoguh it is malignant. Here, the mistake of the classifier could certainly cost you your life.\n",
    "</details>\n",
    "\n",
    "It turns out there is a useful tool called a *confusion matrix* that handily lays it out for you.\n",
    "\n",
    "\\* It's worth putting some effort into what we call positives and negatives. Here, we use the term positive to mean 1 (benign) and negative to mean 0 (malign). However, anyone that has had a test of any sort knows that positive usually means that you have the disease (in our case, malign tumor). A lot of data science is communicating results, so something worth thinking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = np.sum(predictions == y_test)/np.size(predictions)\n",
    "print(accuracy)\n",
    "\n",
    "matrix = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "plt.title('Confusion matrix for our classifier')\n",
    "plt.show(matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, we used two variables (radius and texture) because it made it possible to visualize. However, there is nothing preventing us from using the rest of the data set columns as part of our features. While visualization in more than 2D is much more difficult, the underlying principles are exactly the same.\n",
    "\n",
    "### Exercise\n",
    "Explore this link which contains a number of classification datasets: https://archive.ics.uci.edu/ml/datasets.php?format=&task=cla&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table. Using an SVM, attempt to predict the class of each sample. Consider the following:\n",
    "- While multiclass classification is possible, limit yourself thus far to datasets that have one class. Make sure that y is set up with two distinct numbers for the classes (e.g. 0 and 1)\n",
    "- Make sure that your dataset does not have missing values. If it does, handle them appropriately (e.g. by substituting it with 0 or with the mean of the column).\n",
    "\n",
    "How does your classifier perform? Report the accuracy and confusion matrix. What could be done to improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "So far, we've worked exclusively with linear decision boundaries. However, usually data is much more complicated (as you should have seen from the exercise). Therefore, we might want to look for something that has higher predictive power. One of the most popular tools in machine learning has been the *neural network*. A neural network is (loosely) inspired by the way neurons are connected and operate. Let's first look at the simplest possible neural network and work our way up:\n",
    "\n",
    "<img src=\"notebook_images/nn_nohidden_new.png\" width=\"500\"  />\n",
    "\n",
    "Neural networks are made up of *layers*. In the image above, the neural network has two layers. The first layer is called the *input layer*, and the last layer is called the *output layer*. Each layer is composed of *neurons*. In the input layer, each neuron is one of the features from the training data. For example, the first neuron might be the mean radius, and the second neuron might be the mean texture. We're going to refer to each neuron by $x_{l, n}$, where $l$ is the number of the layer and $n$ is the number of the neuron within the layer. Using this notation, mean radius = $x_{1, 1}$.\n",
    "\n",
    "Each neuron in the input layer is connected to *every* neuron in the following layer by a *weight*. We will denote each weight by $w_{l, i, o}$, where $l$ is the layer at the tail of the arrow, $i$ is the number of the neuron at the tail of the arrow and $o$ is the number of the neuron at the head of the arrow. In the figure above, the color of the arrow corresponds to the value of the weight (blue means negative, red means positive). Every neuron in subsequent layers is equal to the sum of the products of the neurons and the weight that connects them. In this case, the neuron in the output layer is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{2, 1} = x_{1, 1}w_{1, 1, 1} + x_{1, 2}w_{1, 2, 1}\n",
    "\\end{equation}\n",
    "\n",
    "Once you have the value of the output layer, you can obtain the predicted classification by applying some function to $x_{2, 1}$. An example of such a function would be similar to what we did previously:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{pred} =\n",
    "\\begin{cases}\n",
    "    1               & \\text{if } x_{2, 1}\\geq 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "So training a neural network is simply finding the weights $w$ that minimize the error. If you are keen-eyed though, you might have realized that the example above is identical to what we were doing before. Therefore, we still have not arrived at a more powerful classifier. Let's try a more complicated neural network.\n",
    "\n",
    "<img src=\"notebook_images/nn_hidden_new.png\" width=\"500\"  />\n",
    "\n",
    "This network is similar to the previous one, but now we have an additional layer in between the input and output layers. All layers that are between input and output are called *hidden layers*, though functionally they behave in exactly the same way. As a quick concept question, **what is the value of $x_{2, 3}$? What about $x_{3, 1}$?**\n",
    "\n",
    "<details>\n",
    "    <summary>ANSWER</summary>\n",
    "    $x_{2, 3} = x_{1, 1}w_{1, 1, 3} + x_{1, 2}w_{1, 2, 3}$ ,  \n",
    "    $x_{3, 1} = x_{2, 1}w_{2, 1, 1} + x_{2, 2}w_{2, 2, 1} + x_{2, 3}w_{2, 3, 1} + x_{2, 4}w_{2, 4, 1} + x_{2, 5}w_{2, 5, 1}$\n",
    "</details>\n",
    "\n",
    "Why are we introducing a hidden layer? Notice that each of the neurons in the hidden layer are essentially acting as linear classifiers. By having a weighted sum of these neurons in the output layer, we are arriving at a function that, by virtue of being a sum of lines is not itself linear. One of the biggest results in machine learning is that, as you increase the number of neurons in the hidden layer, you can approximate *any* function, meaning that you can always make a more powerful classifier by just adding more neurons. However, usually this is not computationally efficient. Rather, most of the time it makes more sense to introduce more hidden layers as opposed to more neurons, as shown in the image below:\n",
    "\n",
    "<img src=\"notebook_images/nn_deep.png\" width=\"500\"  />\n",
    "\n",
    "The added \"depth\" as a result of more layers is where the \"deep\" in \"deep learning\" comes. As computational resources have increased, scientists have increasingly resorted to adding more and more layers to their neural networks. \n",
    "\n",
    "### Exercise\n",
    "Let's start thinking about the LADI dataset. We want to use deep learning to detect whether there is flooding in the image.\n",
    "- Using the paradigm above, what would be the features of the dataset? What are the targets?\n",
    "- Propose a way that you would input the features into a neural network?\n",
    "- What problems might you run into if you directly put an image into the neural network above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "Convolution is a fairly old technique in image processing. Convolution works by multiplying a matrix (called a *kernel*) with portions of the image and adding up the result. An animation of how this works is shown below. The result of applying different kernels is also shown:\n",
    "\n",
    "<img src=\"notebook_images/convolution_anim.gif\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/convolution.png\" width=\"500\"  />\n",
    "\n",
    "As you can see, different kernels can pick up different features depending on the specific coefficients of the kernel. In the late 1980s, scientists first thought to use neural networks to learn the kernel values themselves, which created the foundation for the *convolutional* neural network (CNN). These complement the hidden layers from the previous architecture with new convolutional layers, which perform convolution on the previous layer. In so doing, these networks conserve the relationship between nearby pixels and perform very well in image classification.\n",
    "\n",
    "<img src=\"notebook_images/cnn.png\" width=\"500\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LADI dataset\n",
    "As we already saw, the LADI dataset contains CAP images as well as crowdsourced labels for a number of different categories. Today we're going to start the multi-day exercise of applying deep learning to the LADI dataset in order to do classification of flooding. This section borrows heavily from the following tutorial: https://github.com/LADI-Dataset/ladi-tutorial\n",
    "\n",
    "### Data reading and cleaning\n",
    "We first have to go through some work to make sure that we only have the components of the dataset that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tsv file with the labels\n",
    "file = pd.read_csv(\"http://ladi.s3-us-west-2.amazonaws.com/Labels/ladi_aggregated_responses_url.tsv\",delimiter='\\t',header='infer')\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip off bracket and comma from the Answer catagory\n",
    "file[\"Answer\"] = file[\"Answer\"].str.strip('[|]')\n",
    "file[\"Answer\"] = file[\"Answer\"].str.split(\",\",expand = True)\n",
    "\n",
    "# Extract labels with damage and infrastructure categories\n",
    "label_damage_infra = file[file['Answer'].str.contains('damage|infrastructure',na=False,case=False)]\n",
    "\n",
    "#Filter out infrastructure label with label 'none'\n",
    "label_clean = label_damage_infra[~label_damage_infra['Answer'].str.contains('none',na=False,case=False)]\n",
    "\n",
    "# Extract data with label does contain 'flood'\n",
    "label_flood = label_clean[label_clean['Answer'].str.contains('flood',na=False,case=False)]\n",
    "\n",
    "# Extract url data with the label does contain 'flood'\n",
    "im_flood_lst = label_flood['url'].unique().tolist()\n",
    "\n",
    "# Extract url data with the label does not contain 'flood'\n",
    "label_notflood = label_damage_infra[~label_damage_infra['url'].isin(im_flood_lst)]\n",
    "im_not_flood_lst = label_notflood['url'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the images we're interested in, we're going to generate the true/false labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ladi_images_metadata.csv\n",
    "metadata = pd.read_csv('http://ladi.s3-us-west-2.amazonaws.com/Labels/ladi_images_metadata.csv')\n",
    "\n",
    "# Generate flood and non-flood metadata\n",
    "flood_metadata = metadata[metadata['url'].isin(im_flood_lst)]\n",
    "not_flood_metadata = metadata[metadata['url'].isin(im_not_flood_lst)]\n",
    "\n",
    "# Generate url and s3_path features into list\n",
    "flood_meta_lst = flood_metadata['url'].tolist()\n",
    "flood_meta_s3_lst = flood_metadata['s3_path'].tolist()\n",
    "\n",
    "not_flood_meta_lst = not_flood_metadata['url'].tolist()\n",
    "not_flood_meta_s3_lst = not_flood_metadata['s3_path'].tolist()\n",
    "\n",
    "# Check how many images do not have metadata but have human labels\n",
    "human_label_only = list(set(im_flood_lst) - set(flood_meta_lst))\n",
    "print(len(human_label_only))\n",
    "human_label_non_flood = list(set(im_not_flood_lst) - set(not_flood_meta_lst))\n",
    "print(len(human_label_non_flood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this exercise, we're going to take just a sample of the overall imagery. We're going to take 100 images that are labeled as flood and 100 that are not:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "# sampling\n",
    "flood_tiny_lst = sample(flood_meta_s3_lst, 100)\n",
    "not_flood_tiny_lst = sample(not_flood_meta_s3_lst, 100)\n",
    "flood_tiny_metadata = metadata[metadata['s3_path'].isin(flood_tiny_lst+not_flood_tiny_lst)]\n",
    "\n",
    "# creating the new datasets\n",
    "flood_data = []\n",
    "for path in flood_tiny_lst:\n",
    "    data_lst = []\n",
    "    data_lst.append(path)\n",
    "    data_lst.append(True)\n",
    "    flood_data.append(data_lst)\n",
    "\n",
    "not_flood_data = []\n",
    "for path in not_flood_tiny_lst:\n",
    "    data_lst = []\n",
    "    data_lst.append(path)\n",
    "    data_lst.append(False)\n",
    "    not_flood_data.append(data_lst)\n",
    "\n",
    "label_data = flood_data+not_flood_data\n",
    "label_df = pd.DataFrame(label_data, columns = ['s3_path', 'label']) \n",
    "\n",
    "flood_tiny_metadata.to_csv('flood_tiny_metadata.csv')\n",
    "label_df.to_csv('flood_tiny_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Take a look at the LADI csv files. Find some classification task that you find meaningful (e.g. detecting a bridge in an image). Create a csv file like the ones above for the metadata and the label. You might have to do some additional steps in data cleaning, depending on your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
